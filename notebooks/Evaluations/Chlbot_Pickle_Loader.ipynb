{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Script only exists as a place to load up all of the pickle files files while I am busy doing other things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import netCDF4 as nc\n",
    "import datetime as dt\n",
    "from salishsea_tools import evaltools as et, viz_tools\n",
    "import gsw \n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib as mpl\n",
    "import matplotlib.dates as mdates\n",
    "import cmocean as cmo\n",
    "import scipy.interpolate as sinterp\n",
    "import pickle\n",
    "import cmocean\n",
    "import json\n",
    "import f90nml\n",
    "from collections import OrderedDict\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "fs=16\n",
    "mpl.rc('xtick', labelsize=fs)\n",
    "mpl.rc('ytick', labelsize=fs)\n",
    "mpl.rc('legend', fontsize=fs)\n",
    "mpl.rc('axes', titlesize=fs)\n",
    "mpl.rc('axes', labelsize=fs)\n",
    "mpl.rc('figure', titlesize=fs)\n",
    "mpl.rc('font', size=fs)\n",
    "mpl.rc('font', family='sans-serif', weight='normal', style='normal')\n",
    "\n",
    "import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "year=2011\n",
    "modelversion='nowcast-green.201905'\n",
    "PATH= '/results2/SalishSea/nowcast-green.201905/'\n",
    "datadir='/ocean/eolson/MEOPAR/obs/WADE/ptools_data/ecology'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTime=pd.read_excel('/ocean/eolson/MEOPAR/obs/WADE/WDE_Data/OlsonSuchyAllen_UBC_PDR_P003790-010721.xlsx',\n",
    "                    engine='openpyxl',sheet_name='EventDateTime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## duplicate Station/Date entries with different times seem to be always within a couple of hours, \n",
    "# so just take the first (next cell)\n",
    "test=dfTime.groupby(['FlightDate','SiteCode'])['TimeDown \\n(Local - PST or PDT)'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['FlightYear', 'FlightMonth', 'FlightDate', 'SiteCode', 'Sampled',\n",
      "       'TimeDown \\n(Local - PST or PDT)', 'FieldComment'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# drop duplicate rows\n",
    "dfTime.drop_duplicates(subset=['FlightDate','SiteCode'],keep='first',inplace=True)\n",
    "print(dfTime.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTime['dtPac']=[dt.datetime.combine(idate, itime) for idate, itime \\\n",
    "         in zip(dfTime['FlightDate'],dfTime['TimeDown \\n(Local - PST or PDT)'])]\n",
    "dfTime['dtUTC']=[et.pac_to_utc(ii) for ii in dfTime['dtPac']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESS STATION LOCATION INFO (based on Parker's code)\n",
    "sta_fn='/ocean/eolson/MEOPAR/obs/WADE/WDE_Data/OlsonSuchyAllen_UBC_PDR_P003790-010721.xlsx'\n",
    "sheetname='Site Info'\n",
    "sta_df =pd.read_excel(sta_fn,engine='openpyxl',sheet_name=sheetname)\n",
    "sta_df.dropna(how='any',subset=['Lat_NAD83 (deg / dec_min)','Long_NAD83 (deg / dec_min)','Station'],inplace=True)\n",
    "sta_df = sta_df.set_index('Station')\n",
    "# get locations in decimal degrees\n",
    "for sta in sta_df.index:\n",
    "    lat_str = sta_df.loc[sta, 'Lat_NAD83 (deg / dec_min)']\n",
    "    lat_deg = float(lat_str.split()[0]) + float(lat_str.split()[1])/60\n",
    "    sta_df.loc[sta,'Lat'] = lat_deg\n",
    "    #\n",
    "    lon_str = sta_df.loc[sta, 'Long_NAD83 (deg / dec_min)']\n",
    "    lon_deg = float(lon_str.split()[0]) + float(lon_str.split()[1])/60\n",
    "    sta_df.loc[sta,'Lon'] = -lon_deg    \n",
    "sta_df.pop('Lat_NAD83 (deg / dec_min)');\n",
    "sta_df.pop('Long_NAD83 (deg / dec_min)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn='/ocean/eolson/MEOPAR/obs/WADE/WDE_Data/OlsonSuchyAllen_UBC_PDR_P003790-010721.xlsx'\n",
    "sheetname='LabChlaPheo'\n",
    "chlPheo =pd.read_excel(fn,engine='openpyxl',sheet_name=sheetname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chlPheo.dropna(how='any',subset=['Date','Station','SamplingDepth'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average over replicates\n",
    "chlPheo2=pd.DataFrame(chlPheo.groupby(['Date','Station','SamplingDepth'],as_index=False).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join to station info (lat/lon)\n",
    "chlPheo3=pd.merge(left=sta_df,right=chlPheo2,how='right',\n",
    "                 left_on='Station',right_on='Station')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join to date/time\n",
    "dfTime['dtUTC']=[et.pac_to_utc(dt.datetime.combine(idate,itime)) for idate,itime in \\\n",
    "                zip(dfTime['FlightDate'],dfTime['TimeDown \\n(Local - PST or PDT)'])]\n",
    "dfTime2=dfTime.loc[:,['FlightDate','SiteCode','dtUTC']]\n",
    "chlPheoFinal=pd.merge(left=chlPheo3,right=dfTime2,how='left',\n",
    "                      left_on=['Date','Station'],right_on=['FlightDate','SiteCode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chlPheoFinal.dropna(how='any',subset=['dtUTC'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chlPheoFinal['Z']=chlPheoFinal['SamplingDepth']\n",
    "chlPheoFinal['Year']=[ii.year for ii in chlPheoFinal['dtUTC']]\n",
    "chlPheoFinal['YD']=et.datetimeToYD(chlPheoFinal['dtUTC'])\n",
    "chlPheoYear=pd.DataFrame(chlPheoFinal.loc[chlPheoFinal.Year==year])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11422, 11469, 5186)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chlPheoFinal),len(chlPheo3),len(dfTime2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load CTD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCTD0=pickle.load(open(os.path.join(datadir,f'Casts_{str(year)}.p'),'rb'))\n",
    "dfCTD=pd.merge(left=sta_df,right=dfCTD0,how='right',\n",
    "             left_on='Station',right_on='Station')\n",
    "dfCTD['dtUTC']=[iiD+dt.timedelta(hours=20) for iiD in dfCTD['Date']] #Does this mean it also has that flaw where we are not sure when the data was collected?\n",
    "dfCTD.rename(columns={'Latitude':'Lat','Longitude':'Lon'},inplace=True)\n",
    "dfCTD['Z']=-1*dfCTD['Z']\n",
    "# Calculate Absolute (Reference) Salinity (g/kg) and Conservative Temperature (deg C) from \n",
    "# Salinity (psu) and Temperature (deg C):\n",
    "press=gsw.p_from_z(-1*dfCTD['Z'],dfCTD['Lat'])\n",
    "dfCTD['SA']=gsw.SA_from_SP(dfCTD['Salinity'],press,\n",
    "                           dfCTD['Lon'],dfCTD['Lat'])\n",
    "dfCTD['CT']=gsw.CT_from_t(dfCTD['SA'],dfCTD['Temperature'],press)\n",
    "\n",
    "dfCTD['Year']=[ii.year for ii in dfCTD['dtUTC']]\n",
    "dfCTD['YD']=et.datetimeToYD(dfCTD['dtUTC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Station', 'Desig', 'Descrip', 'Basin', '*Max_Depth', 'Lat', 'Lon',\n",
       "       'Salinity', 'Temperature', 'Sigma', 'Chl', 'DO', 'Turb', 'Z', 'Date',\n",
       "       'dtUTC', 'SA', 'CT', 'Year', 'YD'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfCTD.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this should be 1:  [1]\n"
     ]
    }
   ],
   "source": [
    "# check that there is never more than one ctd cast per station per day:\n",
    "test=dfCTD.groupby(['Station','Year','YD','Z']).count()\n",
    "print('this should be 1: ',test['Date'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Chlorophyll matched dataset with added CT and SA from CTD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpCTDvar(sta,yr,yd,ztarget,ctdvar):\n",
    "    ctdlocs=(dfCTD.Station==sta)&(dfCTD.Year==yr)&(dfCTD.YD==yd)\n",
    "    if np.sum(ctdlocs)==0:\n",
    "        print(f'Warning: Station {sta}, Year {yr}, year day {yd} not found in dfCTD')\n",
    "        return np.nan\n",
    "    else:\n",
    "        val=np.interp(ztarget,dfCTD.loc[ctdlocs,['Z']].values.flatten(),\n",
    "                  dfCTD.loc[ctdlocs,[ctdvar]].values.flatten())\n",
    "        return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4,  66, 116, 130, 159, 213, 249, 292, 355])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfCTD.loc[dfCTD.Station=='PSS019']['YD'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([193, 242, 271, 334,  83, 109, 137, 173, 207, 234, 258, 284, 314,\n",
       "       354,  23,  57, 141, 170, 199, 232, 362,  51,  88, 115, 176, 204,\n",
       "       233, 336,  87, 139, 174, 196, 231, 309,  50,  70, 124, 153, 223,\n",
       "       265, 300,  34,  68, 108, 130, 158, 206, 235, 255, 306,  18,  45,\n",
       "        86, 107, 135, 156, 226, 254, 275,  31,  92, 121, 155, 197, 219,\n",
       "       281, 323, 346,   7,  35,  72,  99, 163, 246, 301, 110, 138, 159,\n",
       "       201, 230,  22,  48,  76, 103, 292,   4,  66, 116, 214, 249, 355,\n",
       "        11,  37,  69, 101, 123, 213, 261, 299, 332,  16,  78,  98, 140,\n",
       "       168, 302, 326, 345,  56, 111, 127, 210, 259,  21,  55, 119, 166,\n",
       "       187, 264, 327, 350,  25,  53,  82, 188, 236, 256,  54,  94, 129,\n",
       "       165, 228, 296, 341,  10, 114, 143, 198, 248, 276, 310, 338,   8,\n",
       "       157, 225, 267, 303, 318])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chlPheoFinal.loc[chlPheoFinal.Station=='PSS019']['YD'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Station PSS019, Year 2011, year day 214 not found in dfCTD\n",
      "Warning: Station PSS019, Year 2011, year day 214 not found in dfCTD\n"
     ]
    }
   ],
   "source": [
    "chlPheoYear['SA']=[interpCTDvar(sta,yr,yd,ztarget,'SA') for sta, yr, yd, ztarget \\\n",
    "           in zip(chlPheoYear['Station'],chlPheoYear['Year'],chlPheoYear['YD'],chlPheoYear['Z'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Station PSS019, Year 2011, year day 214 not found in dfCTD\n",
      "Warning: Station PSS019, Year 2011, year day 214 not found in dfCTD\n"
     ]
    }
   ],
   "source": [
    "chlPheoYear['CT']=[interpCTDvar(sta,yr,yd,ztarget,'CT') for sta, yr, yd, ztarget \\\n",
    "           in zip(chlPheoYear['Station'],chlPheoYear['Year'],chlPheoYear['YD'],chlPheoYear['Z'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up variables for model-data matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = dt.datetime(year,1,1)\n",
    "end_date = dt.datetime(year,12,31)\n",
    "flen=1 # number of days per model output file. always 1 for 201905 and 201812 model runs\n",
    "namfmt='nowcast' # for 201905 and 201812 model runs, this should always be 'nowcast'\n",
    "filemap={'diatoms':'ptrc_T','ciliates':'ptrc_T','flagellates':'ptrc_T','votemper':'grid_T','vosaline':'grid_T'}\n",
    "fdict={'ptrc_T':1,'grid_T':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Lat,Lon)= 47.21342666666666 -123.07765  not matched to domain\n"
     ]
    }
   ],
   "source": [
    "data_Pheo=et.matchData(chlPheoYear,filemap,fdict,start_date,end_date,'nowcast',PATH,1,quiet=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Saving data as Pickle files to be used in the summary file\n",
    "saveloc='/ocean/kflanaga/MEOPAR/savedData'\n",
    "with open(os.path.join(saveloc,f'data_Pheo_{modelversion}_{year}.pkl'),'wb') as hh:\n",
    "    pickle.dump(data_Pheo,hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py39)",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
